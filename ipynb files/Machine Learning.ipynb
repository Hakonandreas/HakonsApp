{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b8905bf",
   "metadata": {},
   "source": [
    "Github: https://github.com/Hakonandreas/HakonsApp\n",
    "\n",
    "Streamlit: https://hakond2dproject.streamlit.app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900ce567",
   "metadata": {},
   "source": [
    "## Log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fbf2fa",
   "metadata": {},
   "source": [
    "Also this time, as all the other parts, I have tried to follow the working car principle, to make sure I always had a working code. I started by copying my connection to Cassandra and MongoDB from my previous tasks. I then changed my API connection from 2021, to 2021-2024. I found it the best to fetch all the data from scratch, to make the indexing easier. Then I sent everything to Cassandra, using spark, which this time took a really long time. I then fetched the columns I needed and sent them to MongoDB. Everything alligning with what has been done i previous tasks. \n",
    "\n",
    "I started by changing my function that retrieves data fom MongoDB through my api, so it also could get consumption data. Then I struggled for a long time, since it made my whole streamlit page crash, everytime i tried to fetch the elhub data. After a while I got it up and running by splitting my elhub function into one for production and one for consumption. \n",
    "\n",
    "Then i started by turning everything from matplotlib to plotly, that I did not change earlier, before I fixed the comments I got from the last felivery. I then got the geojsn file and started working on the map. I tried using different styles, but ended with folium. I struggled a bit with the colors matching the functionality, but with some help with AI I managed in the end. I then put my snowdrift file up and made that one work using session state for the click. After it worked i merged it into my map page, as I found it the most intuitive. \n",
    "\n",
    "Then I worked with the SWC before starting on the SARIMAX. I struggeled with the SARIMAX, and therefore colaborated with a fellow student to make our codes work. Then I selected the bonus task to also plot the monthly snow drift. I found it the most intuitive to plot them as a common line plot under. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77ea53c",
   "metadata": {},
   "source": [
    "## AI usage\n",
    "First, I generally have copilot enabled in VS code to efficiently write code. In addition, I used ChatGPT to clarify concepts, create parts of the code and debug errors. I made sure to be actively involved in the code, and only used AI to assist me in the process. I also made sure to review all code generated by AI. \n",
    "\n",
    "More concretely tasks where I used AI was to understand what was happening when my streamlit constantly crashed. Then I argued a lot with ChatGPT to get my map working. My last point were I used AI was to fix my own code, was when working with my fellow student, to merge the code that worked for each of us, so that we could compile it into a code that included everything that worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "996a1819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_timestamp, row_number, lit, col, to_date, expr\n",
    "from pyspark.sql.window import Window\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from cassandra.cluster import Cluster\n",
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "import toml\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1665552",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HADOOP_HOME\"] = \"C:/Hadoop/hadoop-3.3.1\"\n",
    "os.environ[\"PYSPARK_HADOOP_VERSION\"] = \"without\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"python\"\n",
    "os.environ['HADOOP_USER_NAME'] = 'hakhol'\n",
    "\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"/Library/Java/JavaVirtualMachines/temurin-11.jdk/Contents/Home\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c49a744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/hakhol/miniconda3/envs/D2D_env/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/hakhol/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/hakhol/.ivy2/jars\n",
      "com.datastax.spark#spark-cassandra-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-55635418-1193-426c-a608-797c6ba53deb;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.datastax.spark#spark-cassandra-connector_2.12;3.5.1 in central\n",
      "\tfound com.datastax.spark#spark-cassandra-connector-driver_2.12;3.5.1 in central\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.11.0 in central\n",
      "\tfound org.apache.cassandra#java-driver-core-shaded;4.18.1 in central\n",
      "\tfound com.datastax.oss#native-protocol;1.5.1 in central\n",
      "\tfound com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 in central\n",
      "\tfound com.typesafe#config;1.4.1 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.26 in central\n",
      "\tfound io.dropwizard.metrics#metrics-core;4.1.18 in central\n",
      "\tfound org.hdrhistogram#HdrHistogram;2.1.12 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.3 in central\n",
      "\tfound org.apache.cassandra#java-driver-mapper-runtime;4.18.1 in central\n",
      "\tfound org.apache.cassandra#java-driver-query-builder;4.18.1 in central\n",
      "\tfound org.apache.commons#commons-lang3;3.10 in central\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.8 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.19 in central\n",
      ":: resolution report :: resolve 1743ms :: artifacts dl 53ms\n",
      "\t:: modules in use:\n",
      "\tcom.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 from central in [default]\n",
      "\tcom.datastax.oss#native-protocol;1.5.1 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector-driver_2.12;3.5.1 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector_2.12;3.5.1 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.8 from central in [default]\n",
      "\tcom.typesafe#config;1.4.1 from central in [default]\n",
      "\tio.dropwizard.metrics#metrics-core;4.1.18 from central in [default]\n",
      "\torg.apache.cassandra#java-driver-core-shaded;4.18.1 from central in [default]\n",
      "\torg.apache.cassandra#java-driver-mapper-runtime;4.18.1 from central in [default]\n",
      "\torg.apache.cassandra#java-driver-query-builder;4.18.1 from central in [default]\n",
      "\torg.apache.commons#commons-lang3;3.10 from central in [default]\n",
      "\torg.hdrhistogram#HdrHistogram;2.1.12 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.3 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.19 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.11.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.26 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   16  |   0   |   0   |   0   ||   16  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-55635418-1193-426c-a608-797c6ba53deb\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 16 already retrieved (0kB/30ms)\n",
      "25/11/16 10:52:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('Elhub_SparkCassandraApp').\\\n",
    "    config('spark.jars.packages', 'com.datastax.spark:spark-cassandra-connector_2.12:3.5.1').\\\n",
    "    config('spark.cassandra.connection.host', 'localhost').\\\n",
    "    config('spark.sql.extensions', 'com.datastax.spark.connector.CassandraSparkExtensions').\\\n",
    "    config('spark.sql.catalog.mycatalog', 'com.datastax.spark.connector.datasource.CassandraCatalog').\\\n",
    "    config('spark.driver.host', '127.0.0.1').\\\n",
    "    config('spark.cassandra.connection.port', '9042').getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fa2ee48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x11c758200>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Connecting to Cassandra\n",
    "cluster = Cluster(['localhost'], port=9042)\n",
    "session = cluster.connect()\n",
    "\n",
    "# Create keyspace (safe to run multiple times)\n",
    "session.execute(\"\"\"\n",
    "    CREATE KEYSPACE IF NOT EXISTS elhub\n",
    "    WITH REPLICATION = {'class':'SimpleStrategy', 'replication_factor':1};\n",
    "\"\"\")\n",
    "\n",
    "time.sleep(1)  # wait for metadata propagation\n",
    "\n",
    "# Drop and create table using fully-qualified name\n",
    "#session.execute(\"DROP TABLE IF EXISTS elhub.production;\")\n",
    "session.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS elhub.production (\n",
    "        pricearea text,\n",
    "        productiongroup text,\n",
    "        starttime timestamp,\n",
    "        ind uuid,\n",
    "        endtime timestamp,\n",
    "        lastupdatedtime timestamp,\n",
    "        quantitykwh double,\n",
    "        PRIMARY KEY ((pricearea, productiongroup), starttime, ind)\n",
    "    );\n",
    "\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76ee439f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinged your deployment. You successfully connected to MongoDB!\n"
     ]
    }
   ],
   "source": [
    "# Connection URI\n",
    "secrets = toml.load(\"../../secrets.toml\")\n",
    "uri = secrets['mongodb']['uri']\n",
    "\n",
    "# Create a new client and connect to the server\n",
    "client = MongoClient(uri, server_api=ServerApi('1'))\n",
    "\n",
    "# Send a ping to confirm a successful connection\n",
    "try:\n",
    "    client.admin.command('ping')\n",
    "    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e41983",
   "metadata": {},
   "source": [
    "### API Connection for Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7132a3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 17856 records for 2021-01\n",
      "Fetched 16128 records for 2021-02\n",
      "Fetched 17832 records for 2021-03\n",
      "Fetched 17280 records for 2021-04\n",
      "Fetched 17856 records for 2021-05\n",
      "Fetched 17976 records for 2021-06\n",
      "Fetched 18600 records for 2021-07\n",
      "Fetched 18600 records for 2021-08\n",
      "Fetched 18000 records for 2021-09\n",
      "Fetched 18625 records for 2021-10\n",
      "Fetched 18000 records for 2021-11\n",
      "Fetched 18600 records for 2021-12\n",
      "Fetched 18600 records for 2022-01\n",
      "Fetched 16800 records for 2022-02\n",
      "Fetched 18575 records for 2022-03\n",
      "Fetched 18000 records for 2022-04\n",
      "Fetched 18600 records for 2022-05\n",
      "Fetched 18000 records for 2022-06\n",
      "Fetched 18600 records for 2022-07\n",
      "Fetched 18600 records for 2022-08\n",
      "Fetched 18000 records for 2022-09\n",
      "Fetched 18625 records for 2022-10\n",
      "Fetched 18000 records for 2022-11\n",
      "Fetched 18600 records for 2022-12\n",
      "Fetched 18600 records for 2023-01\n",
      "Fetched 16800 records for 2023-02\n",
      "Fetched 18575 records for 2023-03\n",
      "Fetched 18000 records for 2023-04\n",
      "Fetched 18600 records for 2023-05\n",
      "Fetched 18000 records for 2023-06\n",
      "Fetched 18600 records for 2023-07\n",
      "Fetched 18600 records for 2023-08\n",
      "Fetched 18000 records for 2023-09\n",
      "Fetched 18625 records for 2023-10\n",
      "Fetched 18000 records for 2023-11\n",
      "Fetched 18600 records for 2023-12\n",
      "Fetched 18600 records for 2024-01\n",
      "Fetched 17400 records for 2024-02\n",
      "Fetched 18575 records for 2024-03\n",
      "Fetched 18000 records for 2024-04\n",
      "Fetched 18600 records for 2024-05\n",
      "Fetched 18000 records for 2024-06\n",
      "Fetched 18600 records for 2024-07\n",
      "Fetched 18600 records for 2024-08\n",
      "Fetched 18000 records for 2024-09\n",
      "Fetched 18625 records for 2024-10\n",
      "Fetched 18000 records for 2024-11\n",
      "Fetched 18600 records for 2024-12\n",
      "\n",
      "Total records fetched: 872953\n",
      "                     endTime            lastUpdatedTime priceArea  \\\n",
      "0  2021-01-01T01:00:00+01:00  2024-12-20T10:35:40+01:00       NO1   \n",
      "1  2021-01-01T02:00:00+01:00  2024-12-20T10:35:40+01:00       NO1   \n",
      "2  2021-01-01T03:00:00+01:00  2024-12-20T10:35:40+01:00       NO1   \n",
      "3  2021-01-01T04:00:00+01:00  2024-12-20T10:35:40+01:00       NO1   \n",
      "4  2021-01-01T05:00:00+01:00  2024-12-20T10:35:40+01:00       NO1   \n",
      "\n",
      "  productionGroup  quantityKwh                  startTime  \n",
      "0           hydro    2507716.8  2021-01-01T00:00:00+01:00  \n",
      "1           hydro    2494728.0  2021-01-01T01:00:00+01:00  \n",
      "2           hydro    2486777.5  2021-01-01T02:00:00+01:00  \n",
      "3           hydro    2461176.0  2021-01-01T03:00:00+01:00  \n",
      "4           hydro    2466969.2  2021-01-01T04:00:00+01:00  \n"
     ]
    }
   ],
   "source": [
    "def fetch_month(year: int, month: int, tz_offset_hours: int = 2, session: requests.Session = None):\n",
    "    \"\"\"Fetch production data for a specific month and year from Elhub API.\"\"\"\n",
    "    \n",
    "    session = session or requests.Session()\n",
    "\n",
    "    # Start and end of the month\n",
    "    start = datetime(year, month, 1)\n",
    "    end = datetime(year + (month == 12), (month % 12) + 1, 1) - timedelta(seconds=1)\n",
    "\n",
    "    start_str = start.strftime(f\"%Y-%m-%dT%H:%M:%S+02:00\")\n",
    "    end_str = end.strftime(f\"%Y-%m-%dT%H:%M:%S+02:00\")\n",
    "\n",
    "    url = \"https://api.elhub.no/energy-data/v0/price-areas\"\n",
    "    params = {\n",
    "        \"dataset\": \"PRODUCTION_PER_GROUP_MBA_HOUR\",\n",
    "        \"startDate\": start_str,\n",
    "        \"endDate\": end_str,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        resp = session.get(url, params=params, timeout=30)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {year}-{month:02d}: {e}\")\n",
    "        return pd.DataFrame()  # return empty DF on error\n",
    "\n",
    "    # Extract production records\n",
    "    all_records = []\n",
    "    for area in data.get(\"data\", []):\n",
    "        records = area.get(\"attributes\", {}).get(\"productionPerGroupMbaHour\", [])\n",
    "        if records:\n",
    "            all_records.extend(records)\n",
    "\n",
    "    df = pd.DataFrame(all_records)\n",
    "    print(f\"Fetched {len(df)} records for {year}-{month:02d}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# Fetch all months from 2021 to 2024 using a Session and skip empty dfs before concat\n",
    "dfs = []\n",
    "with requests.Session() as sess:\n",
    "    for year in range(2021, 2025):   # loop over 2021, 2022, 2023, 2024\n",
    "        for month in range(1, 13):   # loop over all months\n",
    "            monthly_df = fetch_month(year, month, session=sess)\n",
    "            if not monthly_df.empty:\n",
    "                dfs.append(monthly_df)\n",
    "\n",
    "if dfs:\n",
    "    df_all = pd.concat(dfs, ignore_index=True)\n",
    "else:\n",
    "    df_all = pd.DataFrame()\n",
    "\n",
    "print(f\"\\nTotal records fetched: {len(df_all)}\")\n",
    "print(df_all.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84c89c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/15 17:13:34 WARN DeprecatedConfigParameter: spark.cassandra.output.throughput_mb_per_sec is deprecated (DSE 6.0.0) and has been automatically replaced with parameter spark.cassandra.output.throughputMBPerSec. \n",
      "25/11/15 17:13:34 WARN DeprecatedConfigParameter: spark.cassandra.output.throughput_mb_per_sec is deprecated (DSE 6.0.0) and has been automatically replaced with parameter spark.cassandra.output.throughputMBPerSec. \n",
      "25/11/15 17:13:35 WARN DeprecatedConfigParameter: spark.cassandra.output.throughput_mb_per_sec is deprecated (DSE 6.0.0) and has been automatically replaced with parameter spark.cassandra.output.throughputMBPerSec. \n",
      "25/11/15 17:13:37 WARN TaskSetManager: Stage 6 contains a task of very large size (23495 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data successfully written to Cassandra!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_timestamp, expr\n",
    "\n",
    "# ---- Create Spark DataFrame ----\n",
    "spark_df = spark.createDataFrame(df_all)\n",
    "\n",
    "# ---- Rename columns to match Cassandra ----\n",
    "col_map = {\n",
    "    \"startTime\": \"starttime\",\n",
    "    \"endTime\": \"endtime\",\n",
    "    \"lastUpdatedTime\": \"lastupdatedtime\",\n",
    "    \"priceArea\": \"pricearea\",\n",
    "    \"productionGroup\": \"productiongroup\",\n",
    "    \"quantityKwh\": \"quantitykwh\",\n",
    "}\n",
    "\n",
    "for old, new in col_map.items():\n",
    "    spark_df = spark_df.withColumnRenamed(old, new)\n",
    "\n",
    "# ---- Generate UUID per row (FAST, distributed, no shuffle) ----\n",
    "spark_df = spark_df.withColumn(\"ind\", expr(\"uuid()\"))\n",
    "\n",
    "# ---- Convert timestamps / ISO strings → timestamp ----\n",
    "spark_df = (\n",
    "    spark_df\n",
    "        .withColumn(\"starttime\", to_timestamp(\"starttime\"))\n",
    "        .withColumn(\"endtime\", to_timestamp(\"endtime\"))\n",
    "        .withColumn(\"lastupdatedtime\", to_timestamp(\"lastupdatedtime\"))\n",
    ")\n",
    "\n",
    "# ---- Repartition by Cassandra partition key ----\n",
    "# Matches PRIMARY KEY ((pricearea, productiongroup), starttime, ind)\n",
    "spark_df = spark_df.repartition(\"pricearea\", \"productiongroup\")\n",
    "\n",
    "# ---- Coalesce reduces number of writers → faster for Cassandra ----\n",
    "spark_df = spark_df.coalesce(32)   # tune based on cluster size\n",
    "\n",
    "# ---- Write to Cassandra ----\n",
    "(\n",
    "    spark_df.write\n",
    "        .format(\"org.apache.spark.sql.cassandra\")\n",
    "        .mode(\"append\")\n",
    "        .options(table=\"production\", keyspace=\"elhub\")\n",
    "        .option(\"spark.cassandra.output.concurrent.writes\", \"1\")   # single writer\n",
    "        .option(\"spark.cassandra.output.batch.size.rows\", \"1\")     # tiny batches\n",
    "        .option(\"spark.cassandra.output.throughput_mb_per_sec\", \"1\")  # throttle write speed\n",
    "        .save()\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(\"✅ Data successfully written to Cassandra!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2033dc50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+-------------------+-----------+\n",
      "|pricearea|productiongroup|          starttime|quantitykwh|\n",
      "+---------+---------------+-------------------+-----------+\n",
      "|      NO1|          other|2021-01-01 00:00:00|        0.0|\n",
      "|      NO1|          other|2021-01-01 00:00:00|        0.0|\n",
      "|      NO1|          other|2021-01-01 01:00:00|        0.0|\n",
      "|      NO1|          other|2021-01-01 01:00:00|        0.0|\n",
      "|      NO1|          other|2021-01-01 02:00:00|        0.0|\n",
      "|      NO1|          other|2021-01-01 02:00:00|        0.0|\n",
      "|      NO1|          other|2021-01-01 03:00:00|        0.0|\n",
      "|      NO1|          other|2021-01-01 03:00:00|        0.0|\n",
      "|      NO1|          other|2021-01-01 04:00:00|        0.0|\n",
      "|      NO1|          other|2021-01-01 04:00:00|        0.0|\n",
      "|      NO1|          other|2021-01-01 05:00:00|        0.0|\n",
      "|      NO1|          other|2021-01-01 05:00:00|        0.0|\n",
      "|      NO1|          other|2021-01-01 06:00:00|        0.0|\n",
      "|      NO1|          other|2021-01-01 06:00:00|        0.0|\n",
      "|      NO1|          other|2021-01-01 07:00:00|        0.0|\n",
      "|      NO1|          other|2021-01-01 07:00:00|        0.0|\n",
      "|      NO1|          other|2021-01-01 08:00:00|        0.0|\n",
      "|      NO1|          other|2021-01-01 08:00:00|        0.0|\n",
      "|      NO1|          other|2021-01-01 09:00:00|        0.0|\n",
      "|      NO1|          other|2021-01-01 09:00:00|        0.0|\n",
      "+---------+---------------+-------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load table from Cassandra\n",
    "spark_df = spark.read \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .options(table=\"production\", keyspace=\"elhub\") \\\n",
    "    .load()\n",
    "\n",
    "# Select only the columns you want\n",
    "selected_df = spark_df.select(\n",
    "    \"pricearea\",\n",
    "    \"productiongroup\",\n",
    "    \"starttime\",\n",
    "    \"quantitykwh\"\n",
    ")\n",
    "\n",
    "# Show a few rows\n",
    "selected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43100d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped existing MongoDB collection.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 1696647 documents into MongoDB\n"
     ]
    }
   ],
   "source": [
    "# Connect to MongoDB\n",
    "db = client[\"elhub_db\"]\n",
    "collection = db[\"production\"]\n",
    "\n",
    "# Drop the existing collection\n",
    "collection.drop()\n",
    "print(\"Dropped existing MongoDB collection.\")\n",
    "\n",
    "# Convert Spark DataFrame to Pandas\n",
    "mongo_df = selected_df.toPandas()\n",
    "\n",
    "# Insert everything into MongoDB\n",
    "records = mongo_df.to_dict(orient='records')\n",
    "result = collection.insert_many(records)\n",
    "print(f\"Inserted {len(result.inserted_ids)} documents into MongoDB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cea850",
   "metadata": {},
   "source": [
    "### API Connection for Consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "606bd1c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x108cd2870>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#session.execute(\"DROP TABLE IF EXISTS elhub.consumption;\")\n",
    "session.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS elhub.consumption (\n",
    "        pricearea text,\n",
    "        consumptiongroup text,\n",
    "        starttime timestamp,\n",
    "        ind uuid,\n",
    "        endtime timestamp,\n",
    "        lastupdatedtime timestamp,\n",
    "        meteringPointCount int,\n",
    "        quantitykwh double,\n",
    "        PRIMARY KEY ((pricearea, consumptiongroup), starttime, ind)\n",
    "    );\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79e90078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 18600 records for 2021-01\n",
      "Fetched 16800 records for 2021-02\n",
      "Fetched 18575 records for 2021-03\n",
      "Fetched 18000 records for 2021-04\n",
      "Fetched 18600 records for 2021-05\n",
      "Fetched 18000 records for 2021-06\n",
      "Fetched 18600 records for 2021-07\n",
      "Fetched 18600 records for 2021-08\n",
      "Fetched 18000 records for 2021-09\n",
      "Fetched 18625 records for 2021-10\n",
      "Fetched 18000 records for 2021-11\n",
      "Fetched 18600 records for 2021-12\n",
      "Fetched 18600 records for 2022-01\n",
      "Fetched 16800 records for 2022-02\n",
      "Fetched 18575 records for 2022-03\n",
      "Fetched 18000 records for 2022-04\n",
      "Fetched 18600 records for 2022-05\n",
      "Fetched 18000 records for 2022-06\n",
      "Fetched 18600 records for 2022-07\n",
      "Fetched 18600 records for 2022-08\n",
      "Fetched 18000 records for 2022-09\n",
      "Fetched 18625 records for 2022-10\n",
      "Fetched 18000 records for 2022-11\n",
      "Fetched 18600 records for 2022-12\n",
      "Fetched 18600 records for 2023-01\n",
      "Fetched 16800 records for 2023-02\n",
      "Fetched 18575 records for 2023-03\n",
      "Fetched 18000 records for 2023-04\n",
      "Fetched 18600 records for 2023-05\n",
      "Fetched 18000 records for 2023-06\n",
      "Fetched 18600 records for 2023-07\n",
      "Fetched 18600 records for 2023-08\n",
      "Fetched 18000 records for 2023-09\n",
      "Fetched 18625 records for 2023-10\n",
      "Fetched 18000 records for 2023-11\n",
      "Fetched 18600 records for 2023-12\n",
      "Fetched 18600 records for 2024-01\n",
      "Fetched 17400 records for 2024-02\n",
      "Fetched 18575 records for 2024-03\n",
      "Fetched 18000 records for 2024-04\n",
      "Fetched 18600 records for 2024-05\n",
      "Fetched 18000 records for 2024-06\n",
      "Fetched 18600 records for 2024-07\n",
      "Fetched 18600 records for 2024-08\n",
      "Fetched 18000 records for 2024-09\n",
      "Fetched 18625 records for 2024-10\n",
      "Fetched 18000 records for 2024-11\n",
      "Fetched 18600 records for 2024-12\n",
      "\n",
      "Total records fetched: 876600\n",
      "  consumptionGroup                    endTime            lastUpdatedTime  \\\n",
      "0            cabin  2021-01-01T01:00:00+01:00  2024-12-20T10:35:40+01:00   \n",
      "1            cabin  2021-01-01T02:00:00+01:00  2024-12-20T10:35:40+01:00   \n",
      "2            cabin  2021-01-01T03:00:00+01:00  2024-12-20T10:35:40+01:00   \n",
      "3            cabin  2021-01-01T04:00:00+01:00  2024-12-20T10:35:40+01:00   \n",
      "4            cabin  2021-01-01T05:00:00+01:00  2024-12-20T10:35:40+01:00   \n",
      "\n",
      "   meteringPointCount priceArea  quantityKwh                  startTime  \n",
      "0              100607       NO1    177071.56  2021-01-01T00:00:00+01:00  \n",
      "1              100607       NO1    171335.12  2021-01-01T01:00:00+01:00  \n",
      "2              100607       NO1    164912.02  2021-01-01T02:00:00+01:00  \n",
      "3              100607       NO1    160265.77  2021-01-01T03:00:00+01:00  \n",
      "4              100607       NO1    159828.69  2021-01-01T04:00:00+01:00  \n"
     ]
    }
   ],
   "source": [
    "def fetch_month(year: int, month: int, tz_offset_hours: int = 2, session: requests.Session = None):\n",
    "    \"\"\"Fetch production data for a specific month and year from Elhub API.\"\"\"\n",
    "    \n",
    "    session = session or requests.Session()\n",
    "\n",
    "    # Start and end of the month\n",
    "    start = datetime(year, month, 1)\n",
    "    end = datetime(year + (month == 12), (month % 12) + 1, 1) - timedelta(seconds=1)\n",
    "\n",
    "    start_str = start.strftime(f\"%Y-%m-%dT%H:%M:%S+02:00\")\n",
    "    end_str = end.strftime(f\"%Y-%m-%dT%H:%M:%S+02:00\")\n",
    "\n",
    "    url = \"https://api.elhub.no/energy-data/v0/price-areas\"\n",
    "    params = {\n",
    "        \"dataset\": \"CONSUMPTION_PER_GROUP_MBA_HOUR\",\n",
    "        \"startDate\": start_str,\n",
    "        \"endDate\": end_str,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        resp = session.get(url, params=params, timeout=30)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {year}-{month:02d}: {e}\")\n",
    "        return pd.DataFrame()  # return empty DF on error\n",
    "\n",
    "    # Extract production records\n",
    "    all_records = []\n",
    "    for area in data.get(\"data\", []):\n",
    "        records = area.get(\"attributes\", {}).get(\"consumptionPerGroupMbaHour\", [])\n",
    "        if records:\n",
    "            all_records.extend(records)\n",
    "\n",
    "    df = pd.DataFrame(all_records)\n",
    "    print(f\"Fetched {len(df)} records for {year}-{month:02d}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# Fetch all months from 2021 to 2024 using a Session and skip empty dfs before concat\n",
    "dfs = []\n",
    "with requests.Session() as sess:\n",
    "    for year in range(2021, 2025):   # loop over 2021, 2022, 2023, 2024\n",
    "        for month in range(1, 13):   # loop over all months\n",
    "            monthly_df = fetch_month(year, month, session=sess)\n",
    "            if not monthly_df.empty:\n",
    "                dfs.append(monthly_df)\n",
    "\n",
    "if dfs:\n",
    "    df_all_con = pd.concat(dfs, ignore_index=True)\n",
    "else:\n",
    "    df_all_con = pd.DataFrame()\n",
    "\n",
    "print(f\"\\nTotal records fetched: {len(df_all_con)}\")\n",
    "print(df_all_con.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1523ee91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/16 12:27:15 WARN DeprecatedConfigParameter: spark.cassandra.output.throughput_mb_per_sec is deprecated (DSE 6.0.0) and has been automatically replaced with parameter spark.cassandra.output.throughputMBPerSec. \n",
      "25/11/16 12:27:15 WARN DeprecatedConfigParameter: spark.cassandra.output.throughput_mb_per_sec is deprecated (DSE 6.0.0) and has been automatically replaced with parameter spark.cassandra.output.throughputMBPerSec. \n",
      "25/11/16 12:27:16 WARN DeprecatedConfigParameter: spark.cassandra.output.throughput_mb_per_sec is deprecated (DSE 6.0.0) and has been automatically replaced with parameter spark.cassandra.output.throughputMBPerSec. \n",
      "25/11/16 12:27:17 WARN TaskSetManager: Stage 6 contains a task of very large size (24910 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 8:===============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Consumption data successfully written to Cassandra!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# ---- Create Spark DataFrame ----\n",
    "con_spark_df = spark.createDataFrame(df_all_con)\n",
    "\n",
    "# ---- Rename columns to match Cassandra consumption table ----\n",
    "col_map = {\n",
    "    \"startTime\": \"starttime\",\n",
    "    \"endTime\": \"endtime\",\n",
    "    \"lastUpdatedTime\": \"lastupdatedtime\",\n",
    "    \"priceArea\": \"pricearea\",\n",
    "    \"consumptionGroup\": \"consumptiongroup\",\n",
    "    \"meteringPointCount\": \"meteringpointcount\", \n",
    "    \"quantityKwh\": \"quantitykwh\",\n",
    "}\n",
    "\n",
    "for old, new in col_map.items():\n",
    "    con_spark_df = con_spark_df.withColumnRenamed(old, new)\n",
    "\n",
    "# ---- Generate UUID per row ----\n",
    "con_spark_df = con_spark_df.withColumn(\"ind\", expr(\"uuid()\"))\n",
    "\n",
    "# ---- Convert timestamps / ISO strings → timestamp ----\n",
    "con_spark_df = (\n",
    "    con_spark_df\n",
    "        .withColumn(\"starttime\", to_timestamp(\"starttime\"))\n",
    "        .withColumn(\"endtime\", to_timestamp(\"endtime\"))\n",
    "        .withColumn(\"lastupdatedtime\", to_timestamp(\"lastupdatedtime\"))\n",
    ")\n",
    "\n",
    "# ---- Repartition by Cassandra partition key for consumption ----\n",
    "# Matches PRIMARY KEY ((pricearea, consumptiontype), starttime, ind)\n",
    "con_spark_df = con_spark_df.repartition(\"pricearea\", \"consumptiongroup\")\n",
    "\n",
    "# ---- Coalesce reduces number of writers → faster for Cassandra ----\n",
    "con_spark_df = con_spark_df.coalesce(32)   # adjust based on cluster size\n",
    "\n",
    "# ---- Write to Cassandra ----\n",
    "(\n",
    "    con_spark_df.write\n",
    "        .format(\"org.apache.spark.sql.cassandra\")\n",
    "        .mode(\"append\")\n",
    "        .options(table=\"consumption\", keyspace=\"elhub\")  # changed table name\n",
    "        .option(\"spark.cassandra.output.concurrent.writes\", \"1\")   # single writer\n",
    "        .option(\"spark.cassandra.output.batch.size.rows\", \"1\")     # tiny batches\n",
    "        .option(\"spark.cassandra.output.throughput_mb_per_sec\", \"1\")  # throttle write speed\n",
    "        .save()\n",
    ")\n",
    "\n",
    "print(\"✅ Consumption data successfully written to Cassandra!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60d368ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+-------------------+-----------+\n",
      "|pricearea|consumptiongroup|          starttime|quantitykwh|\n",
      "+---------+----------------+-------------------+-----------+\n",
      "|      NO3|           cabin|2021-01-01 00:00:00|   72063.35|\n",
      "|      NO3|           cabin|2021-01-01 01:00:00|  71502.945|\n",
      "|      NO3|           cabin|2021-01-01 02:00:00|   69903.34|\n",
      "|      NO3|           cabin|2021-01-01 03:00:00|    68603.0|\n",
      "|      NO3|           cabin|2021-01-01 04:00:00|   68606.54|\n",
      "|      NO3|           cabin|2021-01-01 05:00:00|  68929.484|\n",
      "|      NO3|           cabin|2021-01-01 06:00:00|   70096.02|\n",
      "|      NO3|           cabin|2021-01-01 07:00:00|   71398.77|\n",
      "|      NO3|           cabin|2021-01-01 08:00:00|   73275.54|\n",
      "|      NO3|           cabin|2021-01-01 09:00:00|   76973.98|\n",
      "|      NO3|           cabin|2021-01-01 10:00:00|    81190.1|\n",
      "|      NO3|           cabin|2021-01-01 11:00:00|    82732.9|\n",
      "|      NO3|           cabin|2021-01-01 12:00:00|   81820.91|\n",
      "|      NO3|           cabin|2021-01-01 13:00:00|   80396.97|\n",
      "|      NO3|           cabin|2021-01-01 14:00:00|  81046.195|\n",
      "|      NO3|           cabin|2021-01-01 15:00:00|   84564.73|\n",
      "|      NO3|           cabin|2021-01-01 16:00:00|   87402.16|\n",
      "|      NO3|           cabin|2021-01-01 17:00:00|   87805.31|\n",
      "|      NO3|           cabin|2021-01-01 18:00:00|   86407.28|\n",
      "|      NO3|           cabin|2021-01-01 19:00:00|   84629.84|\n",
      "+---------+----------------+-------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load table from Cassandra\n",
    "consumption_spark_df = spark.read \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .options(table=\"consumption\", keyspace=\"elhub\") \\\n",
    "    .load()\n",
    "\n",
    "# Select only the columns you want\n",
    "con_selected_df = consumption_spark_df.select(\n",
    "    \"pricearea\",\n",
    "    \"consumptiongroup\",\n",
    "    \"starttime\",\n",
    "    \"quantitykwh\"\n",
    ")\n",
    "\n",
    "# Show a few rows\n",
    "con_selected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c7f6905",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 876600 documents into MongoDB\n"
     ]
    }
   ],
   "source": [
    "# Connect to MongoDB\n",
    "db = client[\"elhub_db\"]\n",
    "collection_con = db[\"consumption\"]\n",
    "\n",
    "# Drop the existing collection\n",
    "#collection_con.drop()\n",
    "#print(\"Dropped existing MongoDB consumption collection.\")\n",
    "\n",
    "# Convert Spark DataFrame to Pandas\n",
    "con_df = con_selected_df.toPandas()\n",
    "\n",
    "# Insert everything into MongoDB\n",
    "records_con = con_df.to_dict(orient='records')\n",
    "result_con = collection_con.insert_many(records_con)\n",
    "\n",
    "print(f\"Inserted {len(result_con.inserted_ids)} documents into MongoDB\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "D2D_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
